import sys
import os
from pathlib import Path
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Base dir = project root (AIPortfolioAgent)
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR / "backend"))  # üëà add backend to sys.path

from backend.loader import load_markdown_docs  # ‚úÖ now Python can find loader.py

STORAGE = BASE_DIR / "storage"
STORAGE.mkdir(exist_ok=True)

# ‚úÖ Load .env from project root
load_dotenv(BASE_DIR / ".env")

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("‚ùå OPENAI_API_KEY not found. Please check your .env file.")

# ‚úÖ Use smaller embedding model (cheaper & quota friendly)
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=api_key
)

# ‚úÖ Define text splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=50
)

print("üìÇ Loading markdown docs...")
docs = load_markdown_docs()
print(f"‚úÖ Loaded {len(docs)} documents")

if not docs:
    print("‚ö†Ô∏è No markdown files found in backend/content/. Add some .md files before building the index.")
    exit()

print("‚úÇÔ∏è Splitting into chunks...")
all_chunks = []
for doc in docs:
    chunks = splitter.split_documents([doc])
    if chunks:
        all_chunks.extend(chunks)
    else:
        # Fallback if doc is very short
        all_chunks.append(doc)

print(f"‚úÖ Created {len(all_chunks)} chunks")

print("‚ö° Building FAISS index...")
vs = FAISS.from_documents(all_chunks, embeddings)
vs.save_local(str(STORAGE / "faiss_index"))
print("‚úÖ Index saved to storage/faiss_index")